from pathlib import Path
text = Path("nlp/summarize.py").read_text(encoding="utf-8")
start = text.index("    async def summarize_message")
text = text[:start] + "    async def _call_chat_completion(self, messages, force_json=False, max_tokens=None):\n        if not self.is_available or not self.chat_url:\n            return None\n\n        payload = {\"messages\": messages, \"temperature\": self.temperature}\n        token_limit = max_tokens if max_tokens is not None else self.max_tokens\n        if self.provider == \"azure\":\n            payload[\"max_output_tokens\"] = token_limit\n        else:\n            payload[\"max_tokens\"] = token_limit\n\n        if self.payload_model:\n            payload[\"model\"] = self.payload_model\n\n        if force_json or self.provider in (\"openai\", \"azure\"):\n            payload[\"response_format\"] = {\"type\": \"json_object\"}\n\n        logger.info(\"[Summarizer][LLM] provider=%s messages=%s\", self.provider, json.dumps(messages, ensure_ascii=False)[:400])\n\n        def _request():\n            resp = self.session.post(self.chat_url, headers=self.headers, json=payload, timeout=40)\n            resp.raise_for_status()\n            return resp.json()\n\n        try:\n            data = asyncio.get_event_loop()\n            data = asyncio.get_running_loop()\n        except RuntimeError:\n            loop = None\n        else:\n            loop = data\n\n        if loop and loop.is_running():\n            resp_json = asyncio.run_coroutine_threadsafe(asyncio.to_thread(_request), loop).result()\n        else:\n            resp_json = _request()\n\n        logger.debug(\"[Summarizer][LLM] response=%s\", json.dumps(resp_json, ensure_ascii=False)[:500])\n        return resp_json\n\n" + text[start:]
Path("nlp/summarize.py").write_text(text, encoding="utf-8")
